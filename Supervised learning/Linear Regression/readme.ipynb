{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Formulation and Algorithm of Linear Regression\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Simple Linear Regression\n",
    "\n",
    "In simple linear regression, we have one independent variable X and one dependent variable y. \n",
    "The relationship between X and y is modeled using a straight line equation:\n",
    "\n",
    "$$ y = mx + b $$\n",
    "\n",
    "where:\n",
    "-$m$ is the slope of the line, representing the change in y for a unit change in X.\n",
    "-$b$ is the y-intercept, the value of y when X is zero.\n",
    "\n",
    "### Multiple Linear Regression\n",
    "\n",
    "In multiple linear regression, we extend the concept to multiple independent variables $$X_1, X_2, ..., X_n$$ influencing the dependent variable y. The relationship is modeled as:\n",
    "\n",
    "$$ y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n $$\n",
    "\n",
    "where:\n",
    "$b_0 $ is the intercept term.\n",
    "$b_1, b_2, ..., b_n $ are the coefficients of the independent variables.\n",
    "\n",
    "## Algorithm: Ordinary Least Squares (OLS)\n",
    "\n",
    "The Ordinary Least Squares method is a common approach for estimating the parameters of a linear regression model. It minimizes the sum of the squared differences between the observed and predicted values of $ y $.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "The cost function, also known as the loss function, measures the difference between the predicted values ( \\hat{y}) and the actual values (y):\n",
    "\n",
    "$$ J(b) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2 $$\n",
    "\n",
    "where:\n",
    "$m$ is the number of training examples.\n",
    "$$ \\hat{y}_i $$ is the predicted value for the $ i $th example.\n",
    "$y_i$ is the actual value for the $i$th example.\n",
    "\n",
    "### Minimization\n",
    "\n",
    "The goal is to find the values of the coefficients \\b_0, b_1, ..., b_n that minimize the cost function J(b). This is typically achieved using optimization techniques like gradient descent.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm used to find the minimum of a function. In the context of linear regression, it updates the coefficients in the direction of the steepest descent of the cost function.\n",
    "\n",
    "$$ b_j := b_j - \\alpha \\frac{\\partial}{\\partial b_j} J(b) $$\n",
    "\n",
    "where:\n",
    "$$ \\alpha $$ is the learning rate, controlling the size of the steps taken during optimization.\n",
    "$$ \\frac{\\partial}{\\partial b_j} J(b) $$ is the partial derivative of the cost function with respect to the $$ j $$th coefficient.\n",
    "\n",
    "### Closed-Form Solution\n",
    "\n",
    "In simple linear regression, a closed-form solution exists for finding the optimal coefficients using matrix operations:\n",
    "\n",
    "$$ b = (X^T X)^{-1} X^T y $$\n",
    "\n",
    "where:\n",
    "$$ X $$ is the design matrix containing the independent variables.\n",
    "$$ y $$ is the vector of dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
